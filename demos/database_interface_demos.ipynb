{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from astroLuSt.database_interface import alerceinterface\n",
    "from astroLuSt.database_interface import eleanorinterface\n",
    "from astroLuSt.database_interface import gaiainterface\n",
    "from astroLuSt.database_interface import lightkurveinterface\n",
    "from astroLuSt.database_interface import simbadinterface\n",
    "\n",
    "import importlib\n",
    "importlib.reload(alerceinterface)\n",
    "importlib.reload(eleanorinterface)\n",
    "importlib.reload(gaiainterface)\n",
    "importlib.reload(lightkurveinterface)\n",
    "importlib.reload(simbadinterface)\n",
    "\n",
    "#style for plotting\n",
    "plt.style.use('astroLuSt.styles.LuSt_style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    \"KIC 5006817\", \"RR Lyr\", \"TV Boo\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimbadDatabaseInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: catalog is None. Corresponding id: G 125-7\n",
      "0    122447138\n",
      "1    159717514\n",
      "2    168709463\n",
      "Name: TIC, dtype: object\n"
     ]
    }
   ],
   "source": [
    "SDI = simbadinterface.SimbadDatabaseInterface()\n",
    "ids = SDI.get_ids(\n",
    "    targets\n",
    ")\n",
    "tics = SDI.df_ids[\"TIC\"]\n",
    "print(tics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EleanorDatabaseInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO(EleanorDatabaseInterface.download()): Extracting chunk 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    6.4s remaining:    0.0s\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\lukas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"C:\\Users\\lukas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nTypeError: 'method' object does not support item assignment\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m      3\u001b[0m EDI \u001b[39m=\u001b[39m eleanorinterface\u001b[39m.\u001b[39mEleanorDatabaseInterface(\n\u001b[0;32m      4\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m      5\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m      6\u001b[0m     )\n\u001b[0;32m      8\u001b[0m \u001b[39m# lc = EDI.extract_source(\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m#     sectors='all',\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m#     source_id={'tic':tics[0]},\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m#     save_kwargs=None,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m EDI\u001b[39m.\u001b[39;49mdownload(\n\u001b[0;32m     17\u001b[0m     sectors\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     18\u001b[0m     source_ids\u001b[39m=\u001b[39;49m[{\u001b[39m'\u001b[39;49m\u001b[39mtic\u001b[39;49m\u001b[39m'\u001b[39;49m:tic} \u001b[39mfor\u001b[39;49;00m tic \u001b[39min\u001b[39;49;00m tics[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]],\n\u001b[0;32m     19\u001b[0m     n_chunks\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     20\u001b[0m     save_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/\u001b[39;49m\u001b[39m'\u001b[39;49m, pd_savefunc\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mto_parquet\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39m# EDI.data_from_eleanor_alltics(\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#     save=\"./data/\",\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#     # save=True,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m#     n_jobs=1, n_chunks=1,\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# )\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Documents\\GitHub\\astroLuSt\\demos\\..\\astroLuSt\\database_interface\\eleanorinterface.py:141\u001b[0m, in \u001b[0;36mEleanorDatabaseInterface.download\u001b[1;34m(self, sectors, source_ids, n_chunks, n_jobs, multi_sectors_kwargs, targetdata_kwargs, save_kwargs, verbose)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m cidx, chunk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunks):\n\u001b[0;32m    134\u001b[0m     almf\u001b[39m.\u001b[39mprintf(\n\u001b[0;32m    135\u001b[0m         msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExtracting chunk \u001b[39m\u001b[39m{\u001b[39;00mcidx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunks)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m    136\u001b[0m         context\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    137\u001b[0m         \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mINFO\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    138\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    139\u001b[0m     )\n\u001b[1;32m--> 141\u001b[0m     res \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49mverbose)(\n\u001b[0;32m    142\u001b[0m         delayed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_source)(\n\u001b[0;32m    143\u001b[0m             sectors\u001b[39m=\u001b[39;49msectors,\n\u001b[0;32m    144\u001b[0m             source_id\u001b[39m=\u001b[39;49msource_id,\n\u001b[0;32m    145\u001b[0m             multi_sectors_kwargs\u001b[39m=\u001b[39;49mmulti_sectors_kwargs,\n\u001b[0;32m    146\u001b[0m             targetdata_kwargs\u001b[39m=\u001b[39;49mtargetdata_kwargs,\n\u001b[0;32m    147\u001b[0m             save_kwargs\u001b[39m=\u001b[39;49msave_kwargs,\n\u001b[0;32m    148\u001b[0m         ) \u001b[39mfor\u001b[39;49;00m idx, source_id \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(chunk)\n\u001b[0;32m    149\u001b[0m     )\n\u001b[0;32m    151\u001b[0m     \u001b[39mprint\u001b[39m(res)\n\u001b[0;32m    153\u001b[0m     \u001b[39m# for idx, source_id in enumerate(chunk):\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[39m#     almf.printf(\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     \u001b[39m#         msg=f'Extracting {source_id} (source {idx+1}/{len(chunk)}, chunk {cidx+1}/{len(chunks)})',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39m#         save_kwargs=save_kwargs,\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m#     )\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "importlib.reload(eleanorinterface)\n",
    "\n",
    "EDI = eleanorinterface.EleanorDatabaseInterface(\n",
    "    n_jobs=2,\n",
    "    verbose=3,\n",
    "    )\n",
    "\n",
    "# lc = EDI.extract_source(\n",
    "#     sectors='all',\n",
    "#     source_id={'tic':tics[0]},\n",
    "#     targetdata_kwargs=dict(do_pca=True, aperture_mode='small'),\n",
    "#     # save_kwargs=dict(directory='./data/', pd_savefunc='to_parquet'),\n",
    "#     save_kwargs=None,\n",
    "# )\n",
    "\n",
    "EDI.download(\n",
    "    sectors=None,\n",
    "    source_ids=[{'tic':tic} for tic in tics[:-1]],\n",
    "    n_chunks=1,\n",
    "    save_kwargs=dict(directory='./data/', pd_savefunc='to_parquet'),\n",
    ")\n",
    "# EDI.data_from_eleanor_alltics(\n",
    "#     save=\"./data/\",\n",
    "#     # save=True,\n",
    "#     redownload=False,\n",
    "#     plot_result=True, save_plot=False,\n",
    "#     sleep=0,\n",
    "#     n_jobs=1, n_chunks=1,\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaiaDatabaseInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GID = gaiainterface.GaiaDatabaseInterface()\n",
    "GID.gaia_crendetials = \"../credentials_gaia.txt\"\n",
    "\n",
    "filter = \"(jobs['phase'] == 'ERROR')\"\n",
    "GID.remove_all_jobs(pd_filter=filter, login_before=False, logout_after=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlerceDatabaseInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting #1/2\n",
      "\n",
      "Extracting #2/2\n",
      "  idx success original error message\n",
      "0   0    True                   None\n",
      "1   1    True                   None\n",
      "\n",
      "Extracting ZTF22abbjfyk (#1/2)\n",
      "WARNING: ./data/ZTF22abbjfyk.csv already exists in ./data/. Ignoring target because \"redownload\" == False \n",
      "\n",
      "Extracting ZTF17aabulmo (#2/2)\n",
      "WARNING: ./data/ZTF17aabulmo.csv already exists in ./data/. Ignoring target because \"redownload\" == False \n",
      "            ztf success                             original error message\n",
      "0  ZTF22abbjfyk   False  WARNING: ./data/ZTF22abbjfyk.csv already exist...\n",
      "1  ZTF17aabulmo   False  WARNING: ./data/ZTF17aabulmo.csv already exist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(alerceinterface)\n",
    "\n",
    "ADI = alerceinterface.AlerceDatabaseInterface()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=np.array([\n",
    "        [10054,\t12.39495833, 27.02213889,],\n",
    "        # [10088,\t353.7751667, np.inf,],#41.10291667,],\n",
    "        [10140,\t16.294625,\t 34.21841667,],\n",
    "        # [10147,\t359.6756667, 41.48880556,],\n",
    "    ]),\n",
    "    columns=['id', 'ra', 'dec']\n",
    ")\n",
    "\n",
    "df_ztf = ADI.crossmerge_by_coordinates(\n",
    "    df_left=df,\n",
    "    ra_colname='ra', dec_colname='dec', radius=1,\n",
    "    sleep=2E-3,\n",
    "    n_jobs=1, verbose=2\n",
    ")\n",
    "\n",
    "print(ADI.df_error_msgs_crossmerge)\n",
    "\n",
    "ADI.download_lightcurves(\n",
    "    df_ztf['oid_ztf'],\n",
    "    save=False,\n",
    "    # save='./data/',\n",
    "    redownload=False,\n",
    "    plot_result=True, save_plot=False, close_plots=False,\n",
    "    sleep=2E-3,\n",
    "    n_jobs=1, verbose=2\n",
    ")\n",
    "print(ADI.df_error_msgs_lcdownload)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightkurveInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(lightkurveinterface)\n",
    "LKI = lightkurveinterface.LightkurveInterface(\n",
    "    tics=tics.astype(np.float64)\n",
    ")\n",
    "\n",
    "LKI.download_lightcurves_tess(\n",
    "    #saving data\n",
    "    save=False,\n",
    "    sectors='all',\n",
    "    quality_expression=\"(datum.quality == 0)\",\n",
    "    include_aperture=False, include_tpf=False,\n",
    "    #plotting\n",
    "    plot_result=True,\n",
    "    aperture_detail=50, ylims=None,\n",
    "    fontsize=16, figsize=(16,9),\n",
    "    save_plot=False,\n",
    "    sleep=0,\n",
    "    n_jobs=1, n_chunks=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(LKI.df_extraction_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrolust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
